{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":4,"outputs":[{"output_type":"stream","text":"/kaggle/input/forest-cover-type-prediction/train.csv\n/kaggle/input/forest-cover-type-prediction/sampleSubmission.csv\n/kaggle/input/forest-cover-type-prediction/test.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"[original kernel](https://www.kaggle.com/nadare/eda-feature-engineering-and-modeling-4th-359)"},{"metadata":{},"cell_type":"markdown","source":"# Summary\nStacking 10 models using LightGBM."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import gaussian_kde\nfrom tqdm import tqdm_notebook\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n\nimport warnings\nimport gc\nfrom multiprocessing import Pool, cpu_count\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nimport lightgbm as lgb\n\nimport lightgbm as lgb\n\nfrom tqdm import tqdm\n\nimport os\nimport gc\nfrom itertools import combinations, chain\nfrom datetime import datetime\n\nfrom itertools import chain, combinations\n\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename_train = '/kaggle/input/forest-cover-type-prediction/train.csv'\nfilename_test = '/kaggle/input/forest-cover-type-prediction/test.csv'\nfilename_submission = '/kaggle/input/forest-cover-type-prediction/sampleSubmission.csv'","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# nadara's kenel"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(filename_train)\ntest_df = pd.read_csv(filename_test)\nsmpsb = pd.read_csv(filename_submission)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA of Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"#### EDA and LeaderBoard Hacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.columns)\nprint()\nprint(train_df.columns[1:11])\n\nfeatures_terrain = train_df.columns[1:11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First of all, let's see the distribution of each variable.\n# You can see there is a big difference in distribution between\n# training data and test data.\n\n\ndef compare_dist(ax, feature, train_df=train_df, test_df=test_df):\n    sns.kdeplot(train_df[feature], label='train', ax=ax)\n    sns.kdeplot(test_df[feature], label='test', ax=ax)\n    \n\ndef numeric_tile(plot_func, features):\n    base_size = 3\n    fig, axes = plt.subplots(3, 4, figsize=(base_size*5, base_size*3))\n    axes = axes.flatten()\n    \n    for i, (ax, col) in enumerate(zip(axes, features)):\n        plot_func(ax, col)\n        ax.set_title(col)\n        \n    plt.tight_layout()\n    \n    \nnumeric_tile(compare_dist, features_terrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For the training data, display the distribution of variables for each target.\n\n# Please pay attention to `Elevation`. The difference between the training \n# data and the teset data distribution is thought to be due to the difference between\n# the propertion of the target variabels in the training data and the test data.\n\ndef compare_target(ax, feature, train_df=train_df, test_df=test_df):\n    sns.kdeplot(train_df.loc[:, feature], label='train', ax=ax)\n    sns.kdeplot(test_df.loc[:, feature], label='test', ax=ax)\n    \n    for target in range(1, 7+1):\n        indices = (train_df['Cover_Type'] == target)\n        sns.kdeplot(train_df.loc[indices, feature], label=target,\n                    alpha=0.6, lw=1, ax=ax)\n        \n        \nnumeric_tile(compare_target, features_terrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LeaderBoard Probing\nHack the distribution of test dataset by submitting 7 times.\n\nReference: [\"Problems occurring during validation\" at 8:30](https://www.coursera.org/learn/competitive-data-science/lecture/8Rp3J/problems-occurring-during-validation)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I was able to obtain the distribution of the test data by submitting prediction data with all the same purpose variables.\n\n\"\"\"\nsmpsb = pd.read_csv('/kaggle/input/forest-cover-type-prediction/sampleSubmission.csv')\n\nfor i in range(1, 7+1):\n    smpsb['Cover_Type'] = i\n    smpsb.to_csv('leader_board_probing_class_{}.csv'.format(i), index=None)\n\"\"\"\n\n# for i in range(1, 7+1):\n#     smpsb['Cover_Type'] = i\n#     smpsb.to_csv('leader_board_probing_class_{}.csv'.format(i), index=None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following is the magic number of this competition.\n# type_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\ntype_ratio = np.array([0.37062, 0.49657, 0.05947, 0.00106, 0.01287, 0.02698, 0.03238])\n\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}\n\nprint('class_weight={}'.format(class_weight))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Demostrate how to make up a fake test data\nCaveat: in the real situation, it only can make up 1 fake feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# By using these numbers, you can mimic the distribution of the test data from the training data.\n\ndef compare_balanced_dist(ax, feature, class_weight=class_weight):\n    min_ = min(train_df[feature].min(),\n              test_df[feature].min())\n    \n    max_ = max(train_df[feature].max(),\n              test_df[feature].max())\n    \n    print('Feature {:35s}: min={}, max={}'.format(feature, min_, max_))\n    \n    X = np.linspace(min_, max_, 1000)\n    \n    sns.kdeplot(train_df[feature], label='train', ax=ax)\n    sns.kdeplot(test_df[feature], label='test', ax=ax)\n    btest = np.zeros(1000)\n    \n    for target in range(1, 7+1):\n        indices = (train_df['Cover_Type'] == target)\n        \n        # Fit train data to a GMM (Gaussian mixture model)\n        gaussian_kernel = gaussian_kde(train_df.loc[indices, feature])\n        \n        # generate PDF using GMM model, then accumlate weighted this PDF.\n        btest += gaussian_kernel(X) * class_weight[target]\n        \n    ax.plot(X, btest, '--', label='balanced', alpha=0.5)\n    ax.legend()\n    \n    \nnumeric_tile(compare_balanced_dist, features_terrain)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# By using the following functions, it is possible to perform almost \n# the same evaluation as the leader board even in the local environment.\n\ndef balanced_accuracy_score(y_true, y_pred, class_weight=class_weight):\n    weights_of_samples = np.apply_along_axis(lambda x: class_weight[x], \n                                                 axis=0, \n                                                 arr=y_true)\n    \n    return accuracy_score(y_true, y_pred,\n                         sample_weight=weights_of_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering 1"},{"metadata":{},"cell_type":"markdown","source":"#### Feature: `Aspect`"},{"metadata":{"trusted":true},"cell_type":"code","source":"sin_ = np.sin(np.pi*train_df['Aspect']/180)\ncos_ = np.cos(np.pi*train_df['Aspect']/180)\n\n# However, if this feature quantity alone, the effect seems to be light.\n\nfig, ax = plt.subplots(figsize=(5,4))\n\n\nfor i in range(1, 7+1):\n    indices = (train_df['Cover_Type'] == i)\n    \n    r = 0.5+0.2*i\n    \n    ax.scatter(cos_[indices]*(r), \n                sin_[indices]*(r), \n                alpha=0.02*r, s=6, label='class {}'.format(i))\n    \n    \nplt.xlim(-2, 3)\nax.legend()\n# plt.savefig('aspect.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Degree to hydrology"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this may be good feature but unfortunally i forgot to add my data\nhydro_h = train_df['Vertical_Distance_To_Hydrology']\nhydro_v = train_df['Horizontal_Distance_To_Hydrology']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(hydro_h, hydro_v, s=1, \n            c=train_df['Cover_Type'], cmap='Set1', alpha=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hydro_arctan = np.arctan((hydro_h+0.0001) / (hydro_v+0.0001))\n\nfor i in range(1, 7+1):\n    indices = (train_df['Cover_Type'] == i)\n    sns.kdeplot(hydro_arctan[indices], label='class_{}'.format(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\n\nax.scatter(hydro_arctan, np.pi*train_df['Slope']/180,\n           c=train_df['Cover_Type'], cmap='Set1', s=1.5, alpha=0.7)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Target Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.filter(regex='Wilder').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\n\ntrain_df.filter(regex='Wilder').sum(axis=0).plot('pie')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# And this is ratio of 'Cover_Type' in each 'Wilderness_area'\n\n# Transform one-hot encodding to label encoding\nwilder = (train_df.filter(regex='Wilder') * np.array([1, 2, 3, 4])).sum(axis=1)\n\nprint(np.sort(wilder.unique()))\nprint()\nprint(wilder.value_counts().sort_index())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Another way to implement transformation of label encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwilderness_cols = ['Wilderness_Area1', 'Wilderness_Area2', \n                       'Wilderness_Area3','Wilderness_Area4']\n\nwilderness_dict = {w: index for index, w in enumerate(wilderness_cols, start=1)}\n\nprint(wilderness_dict)\n\ntrain_df[wilderness_cols].idxmax(axis=1).map(wilderness_dict).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(8,8))\naxes = axes.flatten()\n\nfor i, ax in enumerate(axes, start=1):\n    indices = (wilder == i)\n    train_df.loc[indices, 'Cover_Type'].value_counts().sort_index().plot('pie', ax=ax)\n    \n    ax.set_title('Wilderness Area {}'.format(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This shows the expression of Soil_Type for the objective variable.\n\nplt.figure(figsize=(12, 4))\n\nsns.heatmap(train_df.iloc[:, -41:].sort_values(by='Cover_Type').iloc[:, :-1].T, cmap='Greys_r')\n\nfor i in np.linspace(0, train_df.shape[0], 7+1)[1:]:\n    plt.axvline(i, c='r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As indicated above, categorical values are considered to have a major role in classification.\n\nTherefore, in order to handle categorical values effectively, the **ratio of object variables in each category** is added as a feature quantity.\n\nIn order to prevent data leakage and not to excessively trust categorical values which have only a small number, we added values for 10 data as prior distribution to each category."},{"metadata":{"trusted":true},"cell_type":"code","source":"def categorical_post_mean(x, type_ratio=type_ratio):\n    p = (x.values) * type_ratio\n    p = p/p.sum()*x.sum() + 10*type_ratio\n    \n    return p/p.sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/forest-cover-type-prediction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/forest-cover-type-prediction/test.csv')\nsmpsb = pd.read_csv('/kaggle/input/forest-cover-type-prediction/sampleSubmission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom itertools import combinations\n\ndef main(train_def, test_df, type_ratio):\n    total_df = pd.concat([train_df.drop(columns=['Cover_Type']),\n                          test_df])\n    \n    # Aspect: Axis Tranformation\n    total_df['Aspect_Sin'] = np.sin(np.pi*total_df['Aspect']/180)\n    total_df['Aspect_Cos'] = np.cos(np.pi*total_df['Aspect']/180)\n    \n    # Hillshape: statistical feature engineering\n    hillshade_col = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \n    # Pick combination of `hillshade_col` with size of 2.\n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + '_add_' + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + '_dif_' + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + '_div_' + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + '_abs_' + col2] = np.abs(total_df[col2] - total_df[col1])\n        \n    \n    total_df['Hillshade_mean'] = total_df[hillshade_col].mean(axis=1)\n    total_df['Hillshade_std'] = total_df[hillshade_col].std(axis=1)\n    total_df['Hillshade_max'] = total_df[hillshade_col].max(axis=1)\n    total_df['Hillshade_min'] = total_df[hillshade_col].min(axis=1)\n    \n    \n    # Hydrology: Axis Transformation\n    total_df['Degree_to_Hydrology'] = ((total_df['Vertical_Distance_To_Hydrology'] + 0.001) /\n                                      (total_df['Horizontal_Distance_To_Hydrology']+0.01))\n    \n    # Horizontal: statistical feature engineering\n    horizontal_col = [\"Horizontal_Distance_To_Hydrology\",\n                      \"Horizontal_Distance_To_Roadways\",\n                      \"Horizontal_Distance_To_Fire_Points\"]\n    \n    for col1, col2 in combinations(hillshade_col, 2):\n        total_df[col1 + \"_add_\" + col2] = total_df[col2] + total_df[col1]\n        total_df[col1 + \"_dif_\" + col2] = total_df[col2] - total_df[col1]\n        total_df[col1 + \"_div_\" + col2] = (total_df[col2]+0.01) / (total_df[col1]+0.01)\n        total_df[col1 + \"_abs_\" + col2] = np.abs(total_df[col2] - total_df[col1])\n        \n    \n    def categorical_post_mean(x, type_ratio=type_ratio):\n        p = (x.values)*type_ratio\n        p = p/p.sum()*x.sum() + 10*type_ratio\n        return p/p.sum()\n    \n    \n    # Wilderness Area: target encoding\n    wilderness_cols = ['Wilderness_Area1', 'Wilderness_Area2', \n                       'Wilderness_Area3','Wilderness_Area4']\n    \n    # Transform one-hot encoding to label encoding\n    wilder = pd.DataFrame([(train_df.iloc[:, 11:15] * np.arange(1, 5)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    \n    wilder.columns = ['Wilder_Type', 'Cover_Type']\n    wilder['one'] = 1\n    \n    piv = wilder.pivot_table(values='one',\n                            index='Wilder_Type',\n                            columns='Cover_Type',\n                            aggfunc='sum').fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp['index'] = piv.sum(axis=1).index\n    tmp.columns = ['Wilder_Type'] + ['Wilder_prob_ctype_{}'.format(i) for i in range(1, 7+1)]\n    \n    tmp['Wilder_Type_count'] = piv.sum(axis=1).values\n    \n    total_df['Wilder_Type'] = (total_df[wilderness_cols]*np.arange(1, len(wilderness_cols)+1)).sum(axis=1)\n    total_df = total_df.merge(tmp, on='Wilder_Type', how='left')\n    \n    for i in range(7):\n        total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Wilder_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n        \n    total_df.loc[:, \"Wilder_Type_count\"] = total_df.loc[:, \"Wilder_Type_count\"].fillna(0)\n    \n      \n    # Soil type: target encoding\n    soil = pd.DataFrame([(train_df.iloc[:, -41:-1] * np.arange(1, 41)).sum(axis=1),\n                          train_df.Cover_Type]).T\n    soil.columns = [\"Soil_Type\", \"Cover_Type\"]\n    soil[\"one\"] = 1\n    piv = soil.pivot_table(values=\"one\",\n                           index=\"Soil_Type\",\n                           columns=\"Cover_Type\",\n                           aggfunc=\"sum\").fillna(0)\n    \n    tmp = pd.DataFrame(piv.apply(categorical_post_mean, axis=1).tolist()).reset_index()\n    tmp[\"index\"] = piv.sum(axis=1).index\n    tmp.columns = [\"Soil_Type\"] + [\"Soil_prob_ctype_{}\".format(i) for i in range(1, 8)]\n    tmp[\"Soil_Type_count\"] = piv.sum(axis=1).values\n    \n    total_df[\"Soil_Type\"] = (total_df.filter(regex=\"Soil\") * np.arange(1, 41)).sum(axis=1)\n    total_df = total_df.merge(tmp, on=\"Soil_Type\", how=\"left\")\n    \n    for i in range(7):\n        total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)] = total_df.loc[:, \"Soil_prob_ctype_{}\".format(i+1)].fillna(type_ratio[i])\n    total_df.loc[:, \"Soil_Type_count\"] = total_df.loc[:, \"Soil_Type_count\"].fillna(0)\n    \n    icol = total_df.select_dtypes(np.int64).columns\n    fcol = total_df.select_dtypes(np.float64).columns\n    total_df.loc[:, icol] = total_df.loc[:, icol].astype(np.int32)\n    total_df.loc[:, fcol] = total_df.loc[:, fcol].astype(np.float32)\n    \n    return total_df\n    \n    \ntotal_df = main(train_df, test_df, type_ratio=np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y.shape)\nprint(X.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Layer 1: KNN probability features and Decision tree feature"},{"metadata":{},"cell_type":"markdown","source":"For the variable created up to the above, the decision tree and the k-nearest neighbor method are applied after narrowing down the number of variables and adding the prediction probability as the feature amount.\n\nI decided the combination of variables to be used last and the setting of parameters based on Multi-class logarithmic loss while considering diversity.\n"},{"metadata":{},"cell_type":"markdown","source":"#### KNN_feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_set =  [['Elevation', 500],\n            ['Horizontal_Distance_To_Roadways', 500],\n            ['Horizontal_Distance_To_Fire_Points', 500],\n            ['Horizontal_Distance_To_Hydrology', 500],\n            ['Hillshade_9am', 500],\n            ['Aspect', 500],\n            ['Hillshade_3pm', 500],\n            ['Slope', 500],\n            ['Hillshade_Noon', 500],\n            ['Vertical_Distance_To_Hydrology', 500],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 200],\n            ['Elevation_PLUS_Aspect', 200],\n            ['Elevation_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 200],\n            ['Elevation_PLUS_Hillshade_9am', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 200],\n            ['Elevation_PLUS_Horizontal_Distance_To_Roadways', 100],\n            ['Elevation_PLUS_Vertical_Distance_To_Hydrology', 200],\n            ['Wilder_Type_PLUS_Elevation', 500],\n            ['Wilder_Type_PLUS_Hillshade_Noon_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Degree_to_Hydrology', 200],\n            ['Wilder_Type_PLUS_Hillshade_9am_div_Hillshade_3pm', 500],\n            ['Wilder_Type_PLUS_Aspect_Cos', 500],\n            ['Hillshade_9am_dif_Hillshade_Noon_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200],\n            ['Hillshade_Noon_PLUS_Hillshade_3pm', 200],\n            ['Hillshade_Noon_add_Hillshade_3pm_PLUS_Hillshade_Noon_dif_Hillshade_3pm', 200]\n           ]\n\n\ndef simple_feature_scores2(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    scl = StandardScaler().fit(X.loc[:, cols])\n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = scl.transform(X.loc[train, cols])\n        X_val = scl.transform(X.loc[val, cols])\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = scl.transform(X_test.loc[:, cols])\n        C = clf(**params)\n        C.fit(scl.transform(X.loc[:, cols]), y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \npreds = []\ntest_preds = []\n\nfor colname, neighbor in tqdm_notebook(all_set):\n    gc.collect()\n    \n    params = {'n_neighbors': neighbor,\n#              'n_jobs': cpu_count()\n             }\n\n    %prun ts, tbs, ls, pred, test_pred = simple_feature_scores2(KNeighborsClassifier, \\\n                                                          colname.split(\"_PLUS_\"), \\\n                                                          test=True, \\\n                                                          **params)\n \n    preds.append(pred)\n    test_preds.append(test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(chain.from_iterable([[col[0] + \"_KNN_{}\".format(i) for i in range(1, 8)] for col in all_set]))\nknn_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\nknn_train_df.columns = cols\nknn_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\nknn_test_df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Layer 1: Decision Tree Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_set = [['Elevation', 4],\n           ['Horizontal_Distance_To_Roadways', 4],\n           ['Horizontal_Distance_To_Fire_Points', 3],\n           ['Horizontal_Distance_To_Hydrology', 4],\n           ['Hillshade_9am', 3],\n           ['Vertical_Distance_To_Hydrology', 3],\n           ['Slope', 4],\n           ['Aspect', 4],\n           ['Hillshade_3pm', 3],\n           ['Hillshade_Noon', 3],\n           ['Degree_to_Hydrology', 3],\n           ['Hillshade_Noon_dif_Hillshade_3pm', 3],\n           ['Hillshade_Noon_abs_Hillshade_3pm', 3],\n           ['Elevation_PLUS_Hillshade_9am_add_Hillshade_Noon', 5],\n           ['Elevation_PLUS_Hillshade_max', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Aspect_Sin_PLUS_Aspect_Cos_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Horizontal_Distance_To_Fire_Points', 5],\n           ['Wilder_Type_PLUS_Elevation', 5],\n           ['Elevation_PLUS_Hillshade_9am', 5],\n           ['Elevation_PLUS_Degree_to_Hydrology', 5],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Roadways', 5],\n           ['Wilder_Type_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Wilder_Type_PLUS_Horizontal_Distance_To_Hydrology', 5],\n           ['Wilder_Type_PLUS_Hillshade_Noon_abs_Hillshade_3pm', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_std', 4],\n           ['Hillshade_9am_PLUS_Hillshade_9am_add_Hillshade_Noon', 4],\n           ['Hillshade_9am_add_Hillshade_Noon_PLUS_Hillshade_Noon_add_Hillshade_3pm', 5]]\n\ndef simple_feature_scores(clf, cols, test=False, **params):\n    scores = []\n    bscores = []\n    lscores = []\n    \n    X_preds = np.zeros((len(y), 7))\n    \n    \n    for train, val in StratifiedKFold(n_splits=10, shuffle=True, random_state=2018).split(X, y):\n        X_train = X.loc[train, cols]\n        X_val = X.loc[val, cols]\n        y_train = y[train]\n        y_val = y[val]\n        C = clf(**params) \n\n        C.fit(X_train, y_train)\n        X_preds[val] = C.predict_proba(X_val)\n        #scores.append(accuracy_score(y_val, C.predict(X_val)))\n        #bscores.append(balanced_accuracy_score(y_val, C.predict(X_val)))\n        #lscores.append(log_loss(y_val, C.predict_proba(X_val), labels=list(range(1, 8))))\n    \n    if test:\n        X_test_select = X_test.loc[:, cols]\n        C = clf(**params)\n        C.fit(X.loc[:, cols], y)\n        X_test_preds = C.predict_proba(X_test_select)\n    else:\n        X_test_preds = None\n    return scores, bscores, lscores, X_preds, X_test_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = []\ntest_preds = []\nfor colname, depth in tqdm(all_set):\n    #print(colname, depth)\n    ts, tbs, ls, pred, test_pred = simple_feature_scores(DecisionTreeClassifier,\n                                                         colname.split(\"_PLUS_\"),\n                                                         test=True,\n                                                         max_depth=depth)\n    preds.append(pred)\n    test_preds.append(test_pred)\n\ncols = list(chain.from_iterable([[col[0] + \"_DT_{}\".format(i) for i in range(1, 8)] for col in all_set]))\ndt_train_df = pd.DataFrame(np.hstack(preds)).astype(np.float32)\ndt_train_df.columns = cols\n\ndt_test_df = pd.DataFrame(np.hstack(test_preds)).astype(np.float32)\ndt_test_df.columns = cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target encoding features(1.2.3)\nte_train_df = total_df.filter(regex=\"ctype\").iloc[:len(train_df)]\nte_test_df = total_df.filter(regex=\"ctype\").iloc[len(train_df):]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_level2 = train_df[[\"Id\"]]\ntest_level2 = test_df[[\"Id\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling\n\n6 models\n\n- without KNN&DT features\n    1. Random Forest Classifier\n    1. PCA & K-nearest Neighbors Classifier\n    1. LightGBM\n\n\n- with KNN & DT features\n    1. Random Forest Classifier\n    1. Logistic Regression\n    1. LightGBM\n\nUsing these learning machines, data for stacking was created using 10-fold cross validation."},{"metadata":{},"cell_type":"markdown","source":"#### without KNN&DT feature\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df[\"Cover_Type\"].values\nX = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1)\ntype_ratio = np.array([0.37053, 0.49681, 0.05936, 0.00103, 0.01295, 0.02687, 0.03242])\nclass_weight = {k: v for k, v in enumerate(type_ratio, start=1)}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Layer 2: Random forest classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"RFC1_col = [\"RFC1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in RFC1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=150,\n                             max_depth=12,\n                             class_weight=class_weight,\n                             n_jobs=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.iloc[train, :]\n    X_val = X.iloc[val, :]\n\n    y_train = y[train]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n    y_val_pred = rfc.predict(X_val)\n    y_val_proba = rfc.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, RFC1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nrfc.fit(X, y)\ntest_level2.loc[:, RFC1_col] = rfc.predict_proba(X_test)\n#smpsb.loc[:, \"Cover_Type\"] = rfc.predict(X_test)\n#smpsb.to_csv(\"RFC1.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lyaer 2: PCA & KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNN1_col = [\"KNN1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNN1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=2, n_jobs=-1)\n\nscl = StandardScaler().fit(X_test.drop(cat_col, axis=1))\nX_scl = scl.transform(X.drop(cat_col, axis=1))\nX_test_scl = scl.transform(X_test.drop(cat_col, axis=1))\npca = PCA(n_components=23).fit(X_test_scl)\nX_pca = pca.transform(X_scl)\nX_test_pca = pca.transform(X_test_scl)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X_pca[train]\n    X_val = X_pca[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    knn.fit(X_train, y_train)\n    y_val_pred = knn.predict(X_val)\n    y_val_proba = knn.predict_proba(X_val)\n    \n    confusion += confusion_matrix(y_val, y_val_pred)    \n    train_level2.loc[val, KNN1_col] = y_val_proba\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n\nknn.fit(X_pca, y)\ntest_level2.loc[:, KNN1_col] = knn.predict_proba(X_test_pca)\n#smpsb.loc[:, \"Cover_Type\"] = knn.predict(X_test_pca)\n#smpsb.to_csv(\"KNN1.csv\", index=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Layer 2: LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"LGBM1_col = [\"LGBM1_{}_proba\".format(i) for i in range(1, 8)]\nfor col in LGBM1_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = X.filter(regex=\"Soil_Type|Wilderness\").columns.tolist()[:-1] + [\"Wilder_Type\"]\ncategorical_feature = [29, 38]\nlgbm_col = X.drop(cat_col[:-2], axis=1).columns.tolist()\nclass_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbm = lgb.LGBMClassifier(n_estimators=15,\n                         num_class=7,\n                         learning_rate=0.1,\n                         bagging_fraction=0.6,\n                         num_boost_round=370,\n                         max_depth=8,\n                         max_cat_to_onehot=40,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, random_state=2434, shuffle=True).split(X, y)):\n    X_train = X.loc[train, lgbm_col]\n    X_val = X.loc[val, lgbm_col]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\n            verbose=50, categorical_feature=categorical_feature)\n\n    y_val_pred = gbm.predict(X_val)\n    y_val_proba = gbm.predict_proba(X_val)\n    \n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n    train_level2.loc[val, LGBM1_col] = y_val_proba\n\n\nX_all = X.loc[:, lgbm_col]\nX_test_lgbm = X_test.loc[:, lgbm_col]\ngbm.fit(X_all, y, verbose=50, categorical_feature=categorical_feature)\ntest_level2.loc[:, LGBM1_col] = gbm.predict_proba(X_test_lgbm)\n#smpsb[\"Cover_Type\"] = gbm.predict(X_test_lgbm)\n#smpsb.to_csv(\"LGBM1.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### with KNN & DT features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_p = pd.concat([knn_train_df, dt_train_df, te_train_df], axis=1).astype(np.float32)\nX_test_p = pd.concat([knn_test_df, dt_test_df, te_test_df.reset_index(drop=True)], axis=1).astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Layer 2: RandomForestClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNNDT_RF_col = [\"KNNDT_RF_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_RF_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_jobs=-1,\n                             n_estimators=200,\n                             max_depth=None,\n                             max_features=.7,\n                             max_leaf_nodes=220,\n                             class_weight=class_weight)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    rfc.fit(X_train, y_train)\n\n    y_pred = rfc.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_RF_col] = rfc.predict_proba(X_val)\n\nrfc.fit(X_p, y)\ntest_level2.loc[:, KNNDT_RF_col] = rfc.predict_proba(X_test_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Layer 2: Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNNDT_LR_col = [\"KNNDT_LR_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LR_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X, y)):\n    X_train = X_p.iloc[train, :]\n    y_train = y[train]\n    X_val = X_p.iloc[val, :]\n    y_val = y[val]\n    lr = LogisticRegression(n_jobs=-1, multi_class=\"multinomial\", C=10**9, solver=\"saga\", class_weight=class_weight)\n    lr.fit(X_train, y_train)\n    y_val_pred = lr.predict(X_val)\n    train_level2.loc[val, KNNDT_LR_col] = lr.predict_proba(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_val_pred))\n    confusion += confusion_matrix(y_val, y_val_pred)\n\nlr.fit(X_p, y)\ntest_level2.loc[:, KNNDT_LR_col] = lr.predict_proba(X_test_p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Layer 2: LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"KNNDT_LGB_col = [\"KNNDT_LGB_{}_proba\".format(i) for i in range(1, 8)]\nfor col in KNNDT_LGB_col:\n    train_level2.loc[:, col] = 0\n    test_level2.loc[:, col] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = total_df[total_df[\"Id\"] <= 15120].drop(\"Id\", axis=1)\nX_test = total_df[total_df[\"Id\"] > 15120].drop(\"Id\", axis=1).reset_index(drop=True)\n\nX_d = pd.concat([X.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_train_df,\n                 dt_train_df], axis=1)\n\nX_test_d = pd.concat([X_test.drop(total_df.filter(regex=\"Type\\d+\").columns, axis=1),\n                 knn_test_df,\n                 dt_test_df], axis=1)\n\nfcol = X_d.select_dtypes(np.float64).columns\nX_d.loc[:, fcol] = X_d.loc[:, fcol].astype(np.float32)\nX_d = X_d.values.astype(np.float32)\nX_test_d.loc[:, fcol] = X_test_d.loc[:, fcol].astype(np.float32)\nX_test_d = X_test_d.values.astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weight_lgbm = {i: v for i, v in enumerate(type_ratio)}\n\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=32,\n                         feature_fraction=0.3,\n                         min_child_samples=20,\n                         learning_rate=0.05,\n                         num_boost_round=430,\n                         max_depth=-1,                         \n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=4,\n                         silent=-1,\n                         verbose=-1)\n\nconfusion = np.zeros((7, 7))\nscores = []\nfor train, val in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=2434).split(X_p, y)):\n    X_train = X_d[train]\n    X_val = X_d[val]\n\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, categorical_feature=[33, 42])\n\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n    confusion += confusion_matrix(y_val, y_pred)\n    train_level2.loc[val, KNNDT_LGB_col] = gbm.predict_proba(X_val)\n    \ngbm.fit(X_d, y, categorical_feature=[33, 42])\ntest_level2.loc[:, KNNDT_LGB_col] = gbm.predict_proba(X_test_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.mean(scores))\nsns.heatmap(confusion, cmap=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ykskks's kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(filename_train)\ntest=pd.read_csv(filename_test)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop columns that have the same value in every row\ntrain.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)\ntest.drop(['Soil_Type7', 'Soil_Type15'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['HF1'] = train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points']\ntrain['HF2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HR1'] = abs(train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['HR2'] = abs(train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['FR1'] = abs(train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['FR2'] = abs(train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\ntrain['ele_vert'] = train.Elevation-train.Vertical_Distance_To_Hydrology\ntrain['Mean_Amenities']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + train.Horizontal_Distance_To_Roadways) / 3  \ntrain['Mean_Fire_Hyd']=(train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology) / 2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['HF1'] = test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points']\ntest['HF2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HR1'] = abs(test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['HR2'] = abs(test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['FR1'] = abs(test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['FR2'] = abs(test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\ntest['ele_vert'] = test.Elevation-test.Vertical_Distance_To_Hydrology \ntest['Mean_Amenities']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology + test.Horizontal_Distance_To_Roadways) / 3  \ntest['Mean_Fire_Hyd']=(test.Horizontal_Distance_To_Fire_Points + test.Horizontal_Distance_To_Hydrology) / 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Id for later use\nId_train=train['Id']\nId_test=test['Id']\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train=train.drop('Cover_Type', axis=1)\ny_train=train['Cover_Type']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### randomforest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare df to store pred proba\nx_train_L2=pd.DataFrame(Id_train)\nx_test_L2=pd.DataFrame(Id_test)\nrf_cul=['rf'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in rf_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nrf=RandomForestClassifier(max_depth=None, max_features=20,n_estimators=500, random_state=1)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    rf.fit(x_train_L1, y_train_L1)\n    y_val_proba=rf.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, rf_cul]=y_val_proba\n\nrf.fit(x_train, y_train)\nx_test_L2.loc[:, rf_cul]=rf.predict_proba(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare df to store pred proba\n#x_train_L2=pd.DataFrame(Id_train)\n#x_test_L2=pd.DataFrame(Id_test)\nlgbm_cul=['lgbm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lgbm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n\nlgbm=lgb.LGBMClassifier(learning_rate=0.3, max_depth=-1, min_child_samples=20, n_estimators=300, num_leaves=200, random_state=1, n_jobs=4)\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train)):\n    x_train_L1=x_train.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    lgbm.fit(x_train_L1, y_train_L1)\n    y_val_proba=lgbm.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lgbm_cul]=y_val_proba\n\nlgbm.fit(x_train, y_train)\nx_test_L2.loc[:, lgbm_cul]=lgbm.predict_proba(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_cul=['lr'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in lr_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \npca=PCA(n_components=40)\nx_train_pca=pd.DataFrame(pca.fit_transform(x_train))\ntest_pca=pd.DataFrame(pca.transform(test))\n\npipeline=Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(C=10, solver='newton-cg', multi_class='multinomial',max_iter=500))])\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, lr_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, lr_cul]=pipeline.predict_proba(test_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_cul=['svm'+str(i+1) for i in range(7)]\n\n#prepare cols to store pred proba\nfor i in svm_cul:\n    x_train_L2.loc[:, i]=0\n    x_test_L2.loc[:, i]=0\n    \n#pca=PCA(n_components=40)\n#x_train_pca=pca.fit_transform(x_train)\n#test_pca=pca.transform(test)\n\npipeline=Pipeline([('scaler', StandardScaler()), ('svm', SVC(C=10, gamma=0.1, probability=True))])\n\n\n#StratifiedKfold to avoid leakage\nfor train_index, val_index in tqdm(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train_pca, y_train)):\n    x_train_L1=x_train_pca.iloc[train_index, :]\n    y_train_L1=y_train.iloc[train_index]\n    x_val_L1=x_train_pca.iloc[val_index, :]\n    y_val_L1=y_train.iloc[val_index]\n\n    pipeline.fit(x_train_L1, y_train_L1)\n    y_val_proba=pipeline.predict_proba(x_val_L1)\n    x_train_L2.loc[val_index, svm_cul]=y_val_proba\n\npipeline.fit(x_train_pca, y_train)\nx_test_L2.loc[:, svm_cul]=pipeline.predict_proba(test_pca)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### stacking"},{"metadata":{},"cell_type":"markdown","source":"#### Level1 summary"},{"metadata":{"trusted":true},"cell_type":"code","source":"# concatenate two data\ntrain_L2 = pd.concat([x_train_L2.iloc[:, 1:].reset_index(drop=True), train_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntest_L2 = pd.concat([x_test_L2.iloc[:, 1:].reset_index(drop=True), test_level2.iloc[:, 1:].reset_index(drop=True)], axis=1)\ntrain_L2.to_csv(\"Wtrain_L2.csv\", index=False)\ntest_L2.to_csv(\"Wtest_L2.csv\", index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# each models score\n\ny = pd.read_csv(\"../input/train.csv\")[\"Cover_Type\"].values\nmodel_scores = {}\ntext = []\n\nfor i in range(10):\n    y_pred = np.argmax(train_L2.iloc[:, 7*i:7*(i+1)].values, axis=1) + 1\n    score = balanced_accuracy_score(y, y_pred)\n    model_scores[cols[i*7]] = score\n    text.append(\"{}\\t{:<.5}\".format(train_L2.columns[i*7], score))\n\nprint(*text[::-1], sep=\"\\n\")\npd.Series(model_scores).plot(kind=\"barh\")\nplt.savefig(\"model_summary.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bagging with k-fold\nscores = []\ngbm = lgb.LGBMClassifier(n_estimators=300,\n                         num_class=8,\n                         num_leaves=25,\n                         learning_rate=5,\n                         min_child_samples=20,\n                         bagging_fraction=.3,\n                         bagging_freq=1,\n                         reg_lambda = 10**4.5,\n                         reg_alpha = 1,\n                         feature_fraction=.2,\n                         num_boost_round=8000,\n                         max_depth=-1,\n                         class_weight=class_weight_lgbm,\n                         device=\"cpu\",\n                         n_jobs=-1,\n                         silent=-1,\n                         verbose=-1)\n\nproba = np.zeros((wtest.shape[0], 7))\nfor train, val in tqdm(StratifiedKFold(n_splits=5, shuffle=True, random_state=2434).split(wtrain, y)):\n    X_train = wtrain[train]\n    X_val = wtrain[val]\n    y_train = y[train]\n    y_val = y[val]\n    gbm.fit(X_train, y_train, verbose=-1, \n            eval_set=[(X_train, y_train), (X_val, y_val)], early_stopping_rounds=20)\n    proba += gbm.predict_proba(wtest) / 10\n    y_pred = gbm.predict(X_val)\n    scores.append(balanced_accuracy_score(y_val, y_pred))\n\nprint(np.mean(scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsmpsb[\"Cover_Type\"] = np.argmax(proba, axis=1) + 1\nsmpsb.to_csv(\"final_submission_bagging.csv\", index=False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}
